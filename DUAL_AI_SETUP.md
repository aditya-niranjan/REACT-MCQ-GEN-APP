# ğŸ¤– Smart Dual AI System

Your MCQ Generator now supports **TWO AI backends** with automatic fallback!

## ğŸ¯ How It Works

The system automatically chooses the best available AI service:

```
1. Check for Gemini API Key â†’ Use Gemini âœ…
2. If no key, check Ollama â†’ Use Ollama âœ…
3. If neither available â†’ Show error âŒ
```

## ğŸ”‘ Option 1: Google Gemini API (Cloud)

**Pros:**
- âš¡ Fast (3-5 seconds)
- â˜ï¸ No local installation needed
- ğŸŒ Works anywhere with internet

**Cons:**
- ğŸ”‘ Requires API key
- ğŸ“¤ Data sent to Google
- ğŸ’° Rate limits on free tier

**Setup:**
1. Get free API key: https://makersuite.google.com/app/apikey
2. Add to `.env`:
```properties
GEMINI_API_KEY=your_api_key_here
```
3. Restart backend â†’ Uses Gemini automatically!

---

## ğŸ  Option 2: Ollama (Local GPU)

**Pros:**
- ğŸ”’ 100% Private & Offline
- ğŸ’° Completely free forever
- ğŸ® Uses your RTX 3050 GPU

**Cons:**
- â±ï¸ Slower (10-30 seconds)
- ğŸ’¾ Requires 4.4GB model download
- ğŸ”§ One-time setup needed

**Setup:**
1. You already have Ollama installed! âœ…
2. You already have Mistral 7B downloaded! âœ…
3. Just start Ollama service:
```powershell
ollama serve
```
4. Remove/comment Gemini key in `.env`
5. Restart backend â†’ Uses Ollama automatically!

---

## ğŸš€ Quick Start

### To Use Gemini (Current Setup):
Your `.env` already has the API key, so it's using Gemini now!

```bash
# Backend already running with Gemini
# Just upload a PDF and generate!
```

### To Switch to Ollama:
```powershell
# 1. Start Ollama in a new terminal
ollama serve

# 2. Remove Gemini key from .env (or comment it out)
# GEMINI_API_KEY=

# 3. Restart backend
cd backend
npm start

# Backend will auto-detect and use Ollama!
```

---

## ğŸ“Š Check Current AI Service

Visit: http://localhost:5000/api/status

You'll see:
```json
{
  "success": true,
  "gemini": {
    "available": true,
    "status": "âœ… API Key configured"
  },
  "ollama": {
    "available": false,
    "status": "âŒ Service not running"
  },
  "current": "gemini"
}
```

---

## ğŸ”„ Automatic Fallback

If Gemini fails (quota exceeded, network error, etc.), the system automatically tries Ollama as backup!

**Example flow:**
```
Upload PDF â†’ Try Gemini â†’ Error (quota exceeded)
            â†“
    Auto-retry with Ollama â†’ Success! âœ…
```

---

## âš™ï¸ Configuration (.env)

```properties
PORT=5000

# Option 1: Gemini (remove this to use Ollama)
GEMINI_API_KEY=your_key_here

# Option 2: Ollama (used if no Gemini key)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=mistral:7b
```

---

## ğŸ® Best Practice

**For Development/Testing:**
- Use Gemini (faster iterations)

**For Production/Privacy:**
- Use Ollama (no costs, private)

**For Reliability:**
- Keep both configured (automatic fallback!)

---

## ğŸ› Troubleshooting

### "No AI service available!"
**Solution:** Either:
- Add `GEMINI_API_KEY` to `.env`, OR
- Start Ollama: `ollama serve`

### Ollama not detected
**Check if running:**
```powershell
curl http://localhost:11434
```
**If error, start it:**
```powershell
ollama serve
```

### Gemini quota exceeded
**Solution:** Automatic fallback to Ollama if running!

---

## ğŸ“ˆ Performance Comparison

| Service | Speed | Privacy | Cost | Internet |
|---------|-------|---------|------|----------|
| **Gemini** | âš¡ 3-5s | âš ï¸ Cloud | ğŸ’° Free tier | âœ… Required |
| **Ollama** | ğŸ¢ 10-30s | ğŸ”’ 100% | ğŸ’š Free | âŒ Offline |

---

## ğŸ‰ You're All Set!

Your backend now intelligently chooses the best AI service available. No manual switching needed!

**Current Status:** Using Gemini API
**To switch:** Remove API key or start Ollama
**To use both:** Keep both running for automatic fallback!
