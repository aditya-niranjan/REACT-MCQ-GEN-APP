PORT=5000

# ========================================
# AI SERVICE CONFIGURATION
# ========================================
# Priority: Ollama (PRIMARY) -> Gemini (FALLBACK)
#
# This app supports two AI services:
# 1. Ollama (Local, Private, Free) - PRIMARY
# 2. Gemini (Cloud, Fast) - FALLBACK

# PRIMARY: Ollama (Local AI - 100% Private & Free)
# Requires: ollama serve (running)
# Download: https://ollama.com/download
# Install model: ollama pull mistral:7b

OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=mistral:7b

# FALLBACK: Gemini API (Cloud - Used only if Ollama fails/unavailable)
# Get your free API key: https://makersuite.google.com/app/apikey

# GEMINI_API_KEY=your_gemini_api_key_here


# ========================================
# SETUP INSTRUCTIONS
# ========================================
# 1. Copy this file to .env
# 2. Uncomment and add your Gemini API key (optional)
# 3. Make sure Ollama is installed and running: ollama serve
# 4. Pull the model: ollama pull mistral:7b
# 5. Never commit the .env file to GitHub!

# ========================================
# TESTING DIFFERENT SERVICES
# ========================================
# Test Ollama only: Keep OLLAMA_ variables, comment GEMINI_API_KEY
# Test Gemini only: Comment OLLAMA_ variables, uncomment GEMINI_API_KEY
# Test both (with fallback): Keep all uncommented


# IMPORTANT:
# to check What happens if ollma is not running, first comment out the OLLAMA_ variables above
# and then run this in terminal to stop ollama service 
# Stop-Process -Name ollama -Force
# and then restart backend server.